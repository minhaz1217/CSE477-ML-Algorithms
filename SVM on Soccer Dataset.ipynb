{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [ ['Wales', 'Scotland', 'Friendly', 'Wales'] ] -> output should be LOSS\n",
    "from sklearn import tree\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "data = [] #to store the data(without the column that contains the class) after reading\n",
    "target = [] # contains the class column\n",
    "featureName = [] # the name of the columns, to use in different situations\n",
    "targetName = [] # the unique values of classlabels\n",
    "className = [] # name of the class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData( ):\n",
    "    \n",
    "    FILENAME = \"soccer.csv\"\n",
    "    COLUMNSTOIGNORE = [0,3,4,6]  # ignoring the useless columns, these columns will not be read\n",
    "    NULLVALUEINDICATOR = [ -100000, \"?\"]\n",
    "    \n",
    "    global data, target, featureName, targetName, className\n",
    "    tx = []\n",
    "    ty = []\n",
    "    labels = []\n",
    "    columnName = []\n",
    "    with open(FILENAME,encoding=\"utf8\") as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for i, rows in enumerate(csv_reader):\n",
    "#             if(i>5000):\n",
    "#                 break #breaking early just for quick runs\n",
    "            tempData = []\n",
    "            for j,data in enumerate(rows): #reading every column info from a row and ignoring the unnecessary columns\n",
    "                if j not in COLUMNSTOIGNORE:\n",
    "                    tempData.append(data)\n",
    "            nullvalueFound = 0\n",
    "            for j in NULLVALUEINDICATOR:\n",
    "                if(j in tempData):\n",
    "                    nullvalueFound = 1\n",
    "                    break\n",
    "            if(nullvalueFound):\n",
    "                continue\n",
    "            if i>0: #0 th row should be the name row\n",
    "                datas = tempData[:-1]  #last column is the label column so seperating those\n",
    "                label = (tempData[-1:]) #taking just he labels\n",
    "                #print(datas, label)\n",
    "                tx.append(datas)\n",
    "                ty.append(label)\n",
    "            else:\n",
    "                columnName.extend(tempData) #copying array\n",
    "    data = tx\n",
    "    target = ty\n",
    "    targetName = np.unique(target).tolist()\n",
    "    featureName = columnName[:-1]\n",
    "    className = columnName[-1:]\n",
    "    print(className)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['home_team_result']\n"
     ]
    }
   ],
   "source": [
    "readData()\n",
    "\n",
    "# print(\"Data In Order: \")\n",
    "# print(\"Target names: \")\n",
    "# print(targetName)\n",
    "# print(\"Features\")\n",
    "# print(featureName)\n",
    "# print(\"Data\")\n",
    "# print(data)\n",
    "# print(\"Targets\")\n",
    "# print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories=None, drop=None,\n",
       "              dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "              n_values=None, sparse=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing categorical data to one hot\n",
    "'''\n",
    "Training the encoder here on all the data read from the file because \n",
    "if we train the encoder from only the traning dataset then when testing,\n",
    "the encoder thus the model might face unknown valus that don't have any \n",
    "one hot value.\n",
    "Like in testingdata there might be a column named Ghana but there is no\n",
    "value named Ghana in training dataset. so the encoder/model won't know any value for Ghana\n",
    "'''\n",
    "from sklearn import preprocessing\n",
    "\n",
    "dataEncoder = preprocessing.OneHotEncoder()\n",
    "dataEncoder.fit(data)  #training the encoder\n",
    "\n",
    "targetEncoder = preprocessing.OneHotEncoder()\n",
    "targetEncoder.fit(target)\n",
    "\n",
    "# print(dataEncoder.categories_[])\n",
    "#generating combined feature name from the one hot conversion\n",
    "\n",
    "# onehotFeatureNames = []\n",
    "# for i, val in enumerate(dataEncoder.categories_):\n",
    "# #     print(featureName[i])\n",
    "#     for j in val:\n",
    "#         onehotFeatureNames.append(\"{}:{}\".format(featureName[i],j))\n",
    "# print(onehotFeatureNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size:  27080 Testing Data Size:  11605\n"
     ]
    }
   ],
   "source": [
    "# seperating training data and testing data\n",
    "# for training we use the variables data, target\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "\n",
    "SPLIT = 70 # means: out of 100, 70 will go to training and 30 will go to testing\n",
    "\n",
    "data, target = shuffle(np.array(data), np.array(target))\n",
    "\n",
    "splitPoint = math.ceil( len(data)*SPLIT / 100.0 )\n",
    "testingData = data[splitPoint:]\n",
    "testingTarget = target[splitPoint:]\n",
    "data = data[:splitPoint]\n",
    "target = target[:splitPoint]\n",
    "\n",
    "print(\"Training Data Size: \", len(data),\"Testing Data Size: \", len(testingData))\n",
    "# print(testingData, testingTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the data using the previously trained encoder\n",
    "oneHotData = dataEncoder.transform(data) #converting the data\n",
    "\n",
    "oneHotTarget = targetEncoder.transform(target)\n",
    "\n",
    "# print(encData.toarray())\n",
    "# print(enc.categories_)\n",
    "# print(eTarget.categories_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spTarget = []\n",
    "for i in target:\n",
    "    spTarget.append(i[0])\n",
    "# print(spTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[    4     5     7 ... 27073 27074 27075]\n",
      "[ 6282  7150 10067]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#support vector machine\n",
    "\n",
    "from sklearn  import svm\n",
    "SVM = svm.SVC(gamma='scale', decision_function_shape='ovo')\n",
    "\n",
    "SVM.fit(oneHotData.toarray(), spTarget)\n",
    "\n",
    "print(SVM.support_vectors_)\n",
    "print(SVM.support_ )\n",
    "print(SVM.n_support_ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [['Wales', 'Scotland', 'Friendly', 'Wales']]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=['Loss'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-76be95e54a0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrialData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataEncoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtrialData\u001b[0m \u001b[1;33m)\u001b[0m  \u001b[1;31m# transforming the given data in to our own one hot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrialData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mpredictionResult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargetEncoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Prediction: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictionResult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36minverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'categories_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    519\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=['Loss'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "\n",
    "# ['Wales', 'Scotland', 'Friendly', 'Wales']\n",
    "trialData = [ ['Wales', 'Scotland', 'Friendly', 'Wales'] ]\n",
    "print(\"Input: \",trialData)\n",
    "trialData = dataEncoder.transform( trialData )  # transforming the given data in to our own one hot\n",
    "prediction = SVM.predict(trialData.toarray())\n",
    "predictionResult = targetEncoder.inverse_transform(prediction)\n",
    "print(\"Prediction: \", predictionResult[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR CALCULATION\n",
    "\n",
    "correctAnswer = 0\n",
    "# print(testingData)\n",
    "testData = dataEncoder.transform( testingData )\n",
    "prediction = SVM.predict(testData.toarray())\n",
    "predictionResult = targetEncoder.inverse_transform(prediction)\n",
    "\n",
    "for i,val in enumerate(predictionResult):\n",
    "    if(testingTarget[i] == predictionResult[i]):\n",
    "        correctAnswer +=1\n",
    "successRate = correctAnswer / len(testingTarget)\n",
    "print(\"Success rate: {}%\".format(successRate*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
